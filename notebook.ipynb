{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Sentence Representations from Question-Answer Pairs\n",
    "\n",
    "In this project, we explore how to use deep NLP to learn fixed length vector representations for variable lengthed \"short\" sentences (on the order of at most around 50 words). We will explore two methods used to learn representations through comparison based learning; Siamese networks that utilize contrastive learning, and Triplet networks that learn an embedding from mined triplets of data samples. We will go into the details of both these implementations in subsequent sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Difficulty With Text Data\n",
    "\n",
    "Unless you have a nicely pre-processed dataset on hand, training an NLP model on text data is a challenging problem for several reasons. Text data collected in the wild is often messy and full of errors---either low-level spelling/grammatical errors or high-level logical/factual errors. It takes a lot of work, and sort of defeats the purpose of fully-autonomous NLP, to have to manually clean text data at all of these levels before training a model. Luckily, there are many libraries that can help us initially tackle cleaning low-level spelling/grammatical errors from text data. Dealing with high-level errors becomes more challenging as they affect how an NLP model trains by introducing biases or even just blatantly incorrect supervision signals. For these errors, we are left to come up with clever training techniques that attempt to counteract these distructive signals during training. Additionally, many NLP tasks such as sentiment analysis or flagging specific samples for content violations require training datasets in which samples need to be manually labeled---an expensive and time consuming operation.\n",
    "\n",
    "For this project, we use question-answer data scraped from an online forum because the \"label\" (answer) for each training sample (question) is free. We use quotes around the word \"label\" because these labels are more like \"soft labels\" since they are responses taken verbatim from the general public. Our goal is to use this data in order to learn fixed length representations of user questions from within a specific topic domain (like \"diabetes\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling User Questions\n",
    "\n",
    "The application of understanding user questions would be the starting point, for example, of an engineer attempting to build a chatbot to automatically handle queries. As an initial step, the engineer might desire a clustering of customer queries into different categories. Since we are assuming for this project that forum-based answers are also available, this could be attempted in a straight foward way by training a sequence-to-sequence model to output the corresponding answer string given a question string as input in order to learn useful latent feature representations. This approach would involve some tricky model design, though, and depending on the size of the vocabulary for your data domain, the model output for generating a sequence could be very large, resulting in a difficult-to-manage learning signal. On the other hand, learning sentence representations could be attempted in an entirely unsupervised manner, perhaps by learning to unscramble augmented question strings or impute missing words. This approach would avoid the massive dimensionality at the output when predicting output sentences directly, but could result in learning signals that bias too heavily towards unexpected features (like indiidual words or low-level gramatical logic). We have noisy data in the form of sometimes-wrong-answers that might provide a useful learning signal (on average) nonetheless, therefore it would be unwise to completely ignore this information. Instead, we can combine the fully-supervised sequence-to-sequence approach and the fully-unsupervised approach in a way that \"meets in the middle\" by using the answer strings to construct comparative based training samples. In the folowing sections, we go into the details of two comparison-based learning approaches, train NLP models using these approaches on question-answer data, and explore sentence representations learned through data visualization and cluster analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison-based Question-Answer Supervision\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3001\n",
      "3001\n",
      "(3001, 512)\n",
      "(3001, 512)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_dir = '/home/dylan/trained_model_files/pytorch/sentence2vec/'\n",
    "\n",
    "# load validation question and answer token lists\n",
    "with open('{}sentence2vec_val_question_tok.txt'.format(data_dir), 'r') as fp:\n",
    "    val_question_tok = [line.strip('\\n') for line in fp]\n",
    "with open('{}sentence2vec_val_answer_tok.txt'.format(data_dir), 'r') as fp:\n",
    "    val_answer_tok = [line.strip('\\n') for line in fp]\n",
    "    \n",
    "# load validation question and answer vectors\n",
    "val_question_vec = np.genfromtxt('{}sentence2vec_val_question_vec.txt'.format(data_dir), delimiter=',')\n",
    "val_answer_vec = np.genfromtxt('{}sentence2vec_val_answer_vec.txt'.format(data_dir), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
